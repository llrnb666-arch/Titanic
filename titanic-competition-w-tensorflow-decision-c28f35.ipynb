{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceType":"competition","sourceId":3136,"databundleVersionId":26502}],"dockerImageVersionId":30458,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Titanic competition with TensorFlow Decision Forests\n\nThis notebook will take you through the steps needed to train a baseline Gradient Boosted Trees Model using TensorFlow Decision Forests and creating a submission on the Titanic competition. \n\nTensorFlow Decision Forests是什么：tensorflow是由google开源的AI框架，而TF-DF是它专门用来处理表格数据的分支。代码里写的是python API，但底层核心计算引擎是C++\n\nGBT：基础单元是二叉树。通过梯度来进行提升。\n\n关于这棵二叉树：rootnode判断性别，第二层节点判断年龄，第三层节点判断船票等级……最终leafnode给出综合性别，年龄，船票后的预测得分。\n\n假设第一棵树预测结果是0.3，但实际情况是1.0（存货），那么就产生了0.7的负梯度。第二棵树的终极目标，不再是预测这名乘客是死是活，而是预测那0.7的误差。\n\nThis notebook shows:\n\n1. How to do some basic pre-processing. For example, the passenger names will be tokenized, and ticket names will be splitted in parts.\n1. How to train a Gradient Boosted Trees (GBT) with default parameters\n1. How to train a GBT with improved default parameters\n1. How to tune the parameters of a GBTs\n1. How to train and ensemble many GBTs","metadata":{}},{"cell_type":"markdown","source":"# Imports dependencies","metadata":{}},{"cell_type":"code","source":"import numpy as np\n#相当于math.h\nimport pandas as pd\n#表格处理神器，底层把每一列数据当成高级的结构体数组\nimport os\n#kaggle跟后台的Linux系统打交道，可以直接访问底层的硬盘路径\n\nimport tensorflow as tf\nimport tensorflow_decision_forests as tfdf\n\nprint(f\"Found TF-DF {tfdf.__version__}\")","metadata":{"execution":{"iopub.status.busy":"2023-04-17T15:32:04.264037Z","iopub.execute_input":"2023-04-17T15:32:04.26451Z","iopub.status.idle":"2023-04-17T15:32:04.272355Z","shell.execute_reply.started":"2023-04-17T15:32:04.264459Z","shell.execute_reply":"2023-04-17T15:32:04.270819Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Load dataset","metadata":{}},{"cell_type":"code","source":"#df就是dataframe（数据框），类似一个高级的二维结构体数组\ntrain_df = pd.read_csv(\"/kaggle/input/titanic/train.csv\")\nserving_df = pd.read_csv(\"/kaggle/input/titanic/test.csv\")\n#将train和test分开处理\n\ntrain_df.head(10)\n#jupyter的每一个cell里，放在最后一行的变量或表达式的返回值会被系统自动捕获并打印","metadata":{"execution":{"iopub.status.busy":"2023-04-17T15:30:04.102889Z","iopub.execute_input":"2023-04-17T15:30:04.10358Z","iopub.status.idle":"2023-04-17T15:30:04.158126Z","shell.execute_reply.started":"2023-04-17T15:30:04.103539Z","shell.execute_reply":"2023-04-17T15:30:04.156877Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Prepare dataset\n\nWe will apply the following transformations on the dataset.\n\n1. Tokenize the names. For example, \"Braund, Mr. Owen Harris\" will become [\"Braund\", \"Mr.\", \"Owen\", \"Harris\"].\n2. Extract any prefix in the ticket. For example ticket \"STON/O2. 3101282\" will become \"STON/O2.\" and 3101282.","metadata":{}},{"cell_type":"markdown","source":"预处理：用python切割字符串\n\n1.df = df.copy()这一步极其关键。查了资料才反应过来，python传表格进函数默认传的是引用（类似c语言中的指针）。如果不copy一份，在函数里乱改就会把外面的原数据彻底破坏掉。\n\n2.x.split(\"\")处理字符串的神器，完美代替c语言里的strtok\n\n3..apply()就像一个高速for循环，把清洗函数挨个作用到表格的每一行上。","metadata":{}},{"cell_type":"code","source":"def preprocess(df):\n    df = df.copy()\n    #类似malloc开辟新空间，防止污染原指针数据\n    \n    def normalize_name(x):\n        #替代c语言中的strcat，直接去标点并重新拼接\n        return \" \".join([v.strip(\",()[].\\\"'\") for v in x.split(\" \")])\n        #\"\".join（数组）可以直接以空格为连接符，瞬间将所有碎词粘合成一个完整的字符串\n    \n    def ticket_number(x):\n        return x.split(\" \")[-1]\n        #[-1]可以直接取数组中的最后一个元素（python的语法糖）\n        \n    def ticket_item(x):\n        items = x.split(\" \")\n        if len(items) == 1:\n            return \"NONE\"\n        return \"_\".join(items[0:-1])\n        #[0:-1]是切片语法，相当于从头取到倒数第二个\n    #因为船票最后的数字大概率只是随机流水号，毫无预测价值\n    \n    df[\"Name\"] = df[\"Name\"].apply(normalize_name)\n    df[\"Ticket_number\"] = df[\"Ticket\"].apply(ticket_number)\n    df[\"Ticket_item\"] = df[\"Ticket\"].apply(ticket_item) \n    #apply相当于直接在底层写好了for循环\n    return df\n    \npreprocessed_train_df = preprocess(train_df)\npreprocessed_serving_df = preprocess(serving_df)\n\npreprocessed_train_df.head(5)","metadata":{"execution":{"iopub.status.busy":"2023-04-17T15:30:09.50201Z","iopub.execute_input":"2023-04-17T15:30:09.502467Z","iopub.status.idle":"2023-04-17T15:30:09.540151Z","shell.execute_reply.started":"2023-04-17T15:30:09.502432Z","shell.execute_reply":"2023-04-17T15:30:09.538948Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Let's keep the list of the input features of the model. Notably, we don't want to train our model on the \"PassengerId\" and \"Ticket\" features.","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"挑出有用的线索（特征工程）\n\n这一步的逻辑很简单：把没用的垃圾数据扔掉\n\n比如passngerid只是个流水号，与存活率没有任何关系，如果喂给模型，可能会导致模型死记硬背“几号死了几号活了”导致过拟合\n\n最重要的防作弊：必须把survived（存货状态）从特征里踢出去！因为这是我们要预测的答案。如果把答案混在线索里喂给AI，这就是典型的data leakage。","metadata":{}},{"cell_type":"code","source":"input_features = list(preprocessed_train_df.columns)\n#list是更高级的malloc动态数组，直接将表格上所有的列名拷贝出来，存在一个可以随时增删的字符串线性表中\ninput_features.remove(\"Ticket\")\n#ticket的价值在preprocess中已经被榨干\ninput_features.remove(\"PassengerId\")\n#passengerid是无价值信息\ninput_features.remove(\"Survived\")\n#survived是我们要预测的目标值Y，不能把它当特征X留在这个数组里喂给模型，否则就是把考试答案写在了题干上\n#input_features.remove(\"Ticket_number\")\n\nprint(f\"Input features: {input_features}\")","metadata":{"execution":{"iopub.status.busy":"2023-04-17T15:30:14.000466Z","iopub.execute_input":"2023-04-17T15:30:14.000868Z","iopub.status.idle":"2023-04-17T15:30:14.00835Z","shell.execute_reply.started":"2023-04-17T15:30:14.000833Z","shell.execute_reply":"2023-04-17T15:30:14.006982Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Convert Pandas dataset to TensorFlow Dataset","metadata":{}},{"cell_type":"markdown","source":"组装数据流水线&简单的NLP切词\n\npandas的表格虽然直观，但底层不适合给tensorflow的底层c++引擎做超高速运算。这里用API把数据强制打包成了dataset格式\n\n另外，tokenize_names这一步很有意思。它把名字切成了单独的单词（token）。这样模型在建树的时候，就能自己发现带有\"Mr.\"和\"Miss\"的人的存活率是否有明显区别了。","metadata":{}},{"cell_type":"code","source":"def tokenize_names(features, labels=None):\n    #None：可以满足传入参数和不传入参数两种情况\n    #features：自变量，labels：因变量\n    #机器学习的本质：把大量的features和已知的labels喂给电脑，让电脑自己去把中间那个极其复杂的方程式给推导出来\n    \"\"\"Divite the names into tokens. TF-DF can consume text tokens natively.\"\"\"\n    features[\"Name\"] =  tf.strings.split(features[\"Name\"])\n    return features, labels\n    #return很巧妙：如果是train，就返回切好的题和答案；如果是test，就返回切好的题\n\ntrain_ds = tfdf.keras.pd_dataframe_to_tf_dataset(preprocessed_train_df,label=\"Survived\").map(tokenize_names)\nserving_ds = tfdf.keras.pd_dataframe_to_tf_dataset(preprocessed_serving_df).map(tokenize_names)","metadata":{"execution":{"iopub.status.busy":"2023-04-17T15:30:17.896317Z","iopub.execute_input":"2023-04-17T15:30:17.896741Z","iopub.status.idle":"2023-04-17T15:30:18.231195Z","shell.execute_reply.started":"2023-04-17T15:30:17.896705Z","shell.execute_reply":"2023-04-17T15:30:18.229792Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Train model with default parameters\n\n### Train model\n\nFirst, we are training a GradientBoostedTreesModel model with the default parameters.","metadata":{}},{"cell_type":"markdown","source":"第一次炼丹：建立baseline\n\n在正式调参前，先用 Google 出厂的默认参数跑一版梯度提升树（Gradient Boosted Trees）。\n\n关于GBT我的数学直觉：刚开始觉得机器学习很玄乎，但联系到最近学的高数就通透了。这个模型不是一次性长成一棵完美的树，而是后面长出的每一棵新树，都在去拟合上一棵树造成的误差（负梯度）。这不就是微积分里，利用偏导数寻找函数值下降最快方向的原理吗？每次顺着梯度下山走一小步，最后走到 Loss 误差最小的谷底。","metadata":{}},{"cell_type":"code","source":"model = tfdf.keras.GradientBoostedTreesModel(\n    #减少不必要的控制台打印\n    verbose=0, # Very few logs\n    features=[tfdf.keras.FeatureUsage(name=n) for n in input_features],\n    exclude_non_specified_features=True, # Only use the features in \"features\"\n    #featureusage配合这一行相当于告诉底层只能用提供的inputfeatures，其他数据一律不准碰\n    random_seed=1234,\n    #固定种子是为了保证实验“可复现”，从而调整其他参数引起的准确率变化一定是由参数引起的\n)\nmodel.fit(train_ds)\n\nself_evaluation = model.make_inspector().evaluation()\nprint(f\"Accuracy: {self_evaluation.accuracy} Loss:{self_evaluation.loss}\")","metadata":{"execution":{"iopub.status.busy":"2023-04-17T15:32:16.580089Z","iopub.execute_input":"2023-04-17T15:32:16.581306Z","iopub.status.idle":"2023-04-17T15:32:17.55132Z","shell.execute_reply.started":"2023-04-17T15:32:16.581257Z","shell.execute_reply":"2023-04-17T15:32:17.55024Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Train model with improved default parameters\n\nNow you'll use some specific parameters when creating the GBT model","metadata":{}},{"cell_type":"markdown","source":"第二次炼丹：解析几何与高等数学\n\n这里代码里有大量的#注释，明显是作者在电脑前反复修改数字、手动试错的痕迹。我也仔细研究了这些“底层齿轮”：\n\nmin_examples: 数据结构里讲二叉树，这就像是树向下递归分裂的 base case（基准条件）\n\nSPARSE_OBLIQUE (倾斜分割): 普通的决策树只能画平行于坐标轴的线去切分数据。开启这个选项后，模型会在多维空间里做向量运算，寻找一个倾斜的超平面（Hyperplane）来切分\n\nshrinkage (学习率): 相结合微积分，这就是模型顺着 Loss 函数负梯度“下山”时的步长（Step Size）。步子设小一点（比如 0.05），下山更稳，不容易直接跨过最低点","metadata":{}},{"cell_type":"code","source":"model = tfdf.keras.GradientBoostedTreesModel(\n    verbose=0, # Very few logs\n    features=[tfdf.keras.FeatureUsage(name=n) for n in input_features],\n    exclude_non_specified_features=True, # Only use the features in \"features\"\n    \n    #num_trees=2000,\n    \n    # Only for GBT.\n    # A bit slower, but great to understand the model.\n    # compute_permutation_variable_importance=True,\n    \n    # Change the default hyper-parameters\n    # hyperparameter_template=\"benchmark_rank1@v1\",\n    \n    #num_trees=1000,\n    #tuner=tuner\n    \n    min_examples=1,\n    #递归结束条件：只要节点里还剩一个样本就继续分裂，极限挖掘数据\n    #对于树太深导致过拟合的问题，后面用别的参数去控制\n    categorical_algorithm=\"RANDOM\",\n\n    shrinkage=0.05,\n    #每一棵树对新残差只贡献0.05，当时越过Loss函数的极小值点\n\n    split_axis=\"SPARSE_OBLIQUE\",\n    sparse_oblique_normalization=\"MIN_MAX\",\n    sparse_oblique_num_projections_exponent=2.0,\n    #超平面分割：将分类问题转化为线性代数运算\n    #传统的决策树只能水平切或者垂直切，而sparse_oblique允许在特征空间中画出“斜着的切面”\n    #比如他不再问“票价是否大于100，而是问票价与仓位等级的加权组合是否超过某个阈值“\n    #模型可以通过斜着切的能力用更短的树捕获更复杂的特征关系\n    \n    num_trees=2000,\n    #validation_ratio=0.0,\n    random_seed=1234,\n    \n)\nmodel.fit(train_ds)\n\nself_evaluation = model.make_inspector().evaluation()\nprint(f\"Accuracy: {self_evaluation.accuracy} Loss:{self_evaluation.loss}\")","metadata":{"execution":{"iopub.status.busy":"2023-04-17T15:06:14.822939Z","iopub.execute_input":"2023-04-17T15:06:14.82343Z","iopub.status.idle":"2023-04-17T15:06:16.103565Z","shell.execute_reply.started":"2023-04-17T15:06:14.82339Z","shell.execute_reply":"2023-04-17T15:06:16.102272Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Let's look at the model and you can also notice the information about variable importance that the model figured out","metadata":{}},{"cell_type":"code","source":"model.summary()","metadata":{"execution":{"iopub.status.busy":"2023-04-17T15:06:16.104878Z","iopub.execute_input":"2023-04-17T15:06:16.105239Z","iopub.status.idle":"2023-04-17T15:06:16.123439Z","shell.execute_reply.started":"2023-04-17T15:06:16.105206Z","shell.execute_reply":"2023-04-17T15:06:16.121993Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Make predictions","metadata":{}},{"cell_type":"markdown","source":"交卷：输出预测结果\n\n终于到了最后一步。模型预测出来的并不是直接的0和1，而是“存活的概率”。模型遍历serving_ds的过程，本质上是让每个乘客在2000棵树的结构体数组里跑一遍指针遍历，最后将叶子节点的survival_score累加\n\n所以这里设置了一个threshold=0.5的及格线。大于0.5的判定为 true，然后用.astype(int)强转成整型的1（这里就和 C 语言的 (int)强转一模一样）\n\n最后用to_csv把内存里的结果存成硬盘上的答题卡文件，关掉index行号","metadata":{}},{"cell_type":"code","source":"def prediction_to_kaggle_format(model, threshold=0.5):\n    proba_survive = model.predict(serving_ds, verbose=0)[:,0]\n    return pd.DataFrame({\n        \"PassengerId\": serving_df[\"PassengerId\"],\n        #保持原有的乘客ID，方便kaggle机器对号入座\n        \n        \"Survived\": (proba_survive >= threshold).astype(int)\n    })\n\ndef make_submission(kaggle_predictions):\n    path=\"/kaggle/working/submission.csv\"\n    kaggle_predictions.to_csv(path, index=False)\n    print(f\"Submission exported to {path}\")\n    \nkaggle_predictions = prediction_to_kaggle_format(model)\nmake_submission(kaggle_predictions)\n!head /kaggle/working/submission.csv","metadata":{"execution":{"iopub.status.busy":"2023-04-17T15:06:16.126575Z","iopub.execute_input":"2023-04-17T15:06:16.126941Z","iopub.status.idle":"2023-04-17T15:06:17.406876Z","shell.execute_reply.started":"2023-04-17T15:06:16.126904Z","shell.execute_reply":"2023-04-17T15:06:17.405022Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Training a model with hyperparameter tunning\n\nHyper-parameter tuning is enabled by specifying the tuner constructor argument of the model. The tuner object contains all the configuration of the tuner (search space, optimizer, trial and objective).\n","metadata":{}},{"cell_type":"markdown","source":"第三次炼丹：直接用算力来穷举\n\n刚才看了作者手动改参数，我还在想 AI 工程师难道是个体力活？直到看到这个RandomSearch模块我才懂了\n\n用 C 语言的眼光看，这其实就是在模型外面套了一个极度优化的、执行 1000 次的超级 for 循环（num_trials=1000）。通过 tuner.choice 传进去一个个数组，让机器自己在几百种组合里抛骰子试错。这让我明白了，在真实的工程中，比起死磕某一个数学公式，如何利用机器庞大的算力去自动化寻找最优解（全局最优节点 vs 局部最大深度），才是真正的降维打击。","metadata":{}},{"cell_type":"code","source":"tuner = tfdf.tuner.RandomSearch(num_trials=1000)\n#开启1000次独立实验的随即搜索\n\ntuner.choice(\"min_examples\", [2, 5, 7, 10])\ntuner.choice(\"categorical_algorithm\", [\"CART\", \"RANDOM\"])\n#在这些候选值里掷骰子\n\nlocal_search_space = tuner.choice(\"growing_strategy\", [\"LOCAL\"])\nlocal_search_space.choice(\"max_depth\", [3, 4, 5, 6, 8])\n#策略1：传统的”按层生长“，限制二叉树的最大深度\n\nglobal_search_space = tuner.choice(\"growing_strategy\", [\"BEST_FIRST_GLOBAL\"], merge=True)\nglobal_search_space.choice(\"max_num_nodes\", [16, 32, 64, 128, 256])\n#策略2：现代的“按质生长”，限制二叉树的总结点数\n#谁的效果好谁就先分叉，虽然这样生成的树往往不对称，但是更高效\n\n#tuner.choice(\"use_hessian_gain\", [True, False])\ntuner.choice(\"shrinkage\", [0.02, 0.05, 0.10, 0.15])\ntuner.choice(\"num_candidate_attributes_ratio\", [0.2, 0.5, 0.9, 1.0])\n\n\ntuner.choice(\"split_axis\", [\"AXIS_ALIGNED\"])\noblique_space = tuner.choice(\"split_axis\", [\"SPARSE_OBLIQUE\"], merge=True)\noblique_space.choice(\"sparse_oblique_normalization\",\n                     [\"NONE\", \"STANDARD_DEVIATION\", \"MIN_MAX\"])\noblique_space.choice(\"sparse_oblique_weights\", [\"BINARY\", \"CONTINUOUS\"])\noblique_space.choice(\"sparse_oblique_num_projections_exponent\", [1.0, 1.5])\n#针对倾斜分割的一系列细则开关\n\n# Tune the model. Notice the `tuner=tuner`.\ntuned_model = tfdf.keras.GradientBoostedTreesModel(tuner=tuner)\ntuned_model.fit(train_ds, verbose=0)\n#将“自动调参机器人”挂载到模型上\n#tuned_model在fit时，不再只炼丹一次，而是疯狂的跑1000遍不同的参数组合\n\ntuned_self_evaluation = tuned_model.make_inspector().evaluation()\nprint(f\"Accuracy: {tuned_self_evaluation.accuracy} Loss:{tuned_self_evaluation.loss}\")\n#提取这1000次实验中胜出的参数组合","metadata":{"execution":{"iopub.status.busy":"2023-04-17T15:23:13.249675Z","iopub.execute_input":"2023-04-17T15:23:13.251376Z","iopub.status.idle":"2023-04-17T15:25:19.611729Z","shell.execute_reply.started":"2023-04-17T15:23:13.251306Z","shell.execute_reply":"2023-04-17T15:25:19.610154Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"In the last line in the cell above, you can see the accuracy is higher than previously with default parameters and parameters set by hand.\n\nThis is the main idea behing hyperparameter tuning.\n\nFor more information you can follow this tutorial: [Automated hyper-parameter tuning](https://www.tensorflow.org/decision_forests/tutorials/automatic_tuning_colab)","metadata":{}},{"cell_type":"markdown","source":"# Making an ensemble\n\nHere you'll create 100 models with different seeds and combine their results\n\nThis approach removes a little bit the random aspects related to creating ML models\n\nIn the GBT creation is used the `honest` parameter. It will use different training examples to infer the structure and the leaf values. This regularization technique trades examples for bias estimates.","metadata":{}},{"cell_type":"markdown","source":"颠覆我对机器学习认知的部分：我原本以为，拿高分就是要找出一个极其完美的“超级模型”。但原来工业界的杀手锏是“打群架”\n\nEnsemble（模型集成）的底层逻辑：\n作者不仅用了刚才的GBT，可能还加了随机森林random forest等其他不同算法训练出来的模型。然后，把这几个模型对同一个乘客预测的存活概率，做了一个简单的加权平均：$P_{final} = \\frac{1}{N}\\sum P_i$\n\n在 C 语言里，这不过是一个极其简单的 for 循环累加求平均值的操作。但从概率论的角度来看，每个模型都有自己的“偏见”（方差），但只要它们犯错的方向不一样，求平均值就能极大程度地抵消误差，让最终的预测结果变得无比稳定。所谓“三个臭皮匠顶个诸葛亮”，大概就是这个数学原理吧","metadata":{}},{"cell_type":"code","source":"predictions = None\nnum_predictions = 0\n#类似c语言中初始化一个指向float数组的指针\n\nfor i in range(100):\n    print(f\"i:{i}\")\n    # Possible models: GradientBoostedTreesModel or RandomForestModel\n    model = tfdf.keras.GradientBoostedTreesModel(\n        verbose=0, # Very few logs\n        features=[tfdf.keras.FeatureUsage(name=n) for n in input_features],\n        exclude_non_specified_features=True, # Only use the features in \"features\"\n        #每次循环都在底层重新实例化一个c++引擎结构体\n\n        #min_examples=1,\n        #categorical_algorithm=\"RANDOM\",\n        ##max_depth=4,\n        #shrinkage=0.05,\n        ##num_candidate_attributes_ratio=0.2,\n        #split_axis=\"SPARSE_OBLIQUE\",\n        #sparse_oblique_normalization=\"MIN_MAX\",\n        #sparse_oblique_num_projections_exponent=2.0,\n        #num_trees=2000,\n        ##validation_ratio=0.0,\n        random_seed=i,\n        #多样性消除误差\n        #前面固定种子是为了调参，这里随机种子是为了集成\n        honest=True,\n    )\n    model.fit(train_ds)\n    \n    sub_predictions = model.predict(serving_ds, verbose=0)[:,0]\n    if predictions is None:\n        predictions = sub_predictions\n    else:\n        predictions += sub_predictions\n    num_predictions += 1\n\npredictions/=num_predictions\n\nkaggle_predictions = pd.DataFrame({\n        \"PassengerId\": serving_df[\"PassengerId\"],\n        \"Survived\": (predictions >= 0.5).astype(int)\n    })\n\nmake_submission(kaggle_predictions)","metadata":{"execution":{"iopub.status.busy":"2023-04-17T15:28:37.557745Z","iopub.execute_input":"2023-04-17T15:28:37.558172Z","iopub.status.idle":"2023-04-17T15:28:52.809698Z","shell.execute_reply.started":"2023-04-17T15:28:37.55813Z","shell.execute_reply":"2023-04-17T15:28:52.808193Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# What is next\n\nIf you want to learn more about TensorFlow Decision Forests and its advanced features, you can follow the official documentation [here](https://www.tensorflow.org/decision_forests) ","metadata":{}}]}